{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Intro to RDDS #\n",
    "\n",
    "In this lab, we will introduce RDDs and Spark's core operations. Along the way, we'll get familiar with pyspark, notebooks, and the SparkContext.\n",
    "\n",
    "Let's start by making sure we have a valid SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10ff0aed0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You should see something like: `<pyspark.context.SparkContext at 0x1015a2690>`\n",
    "\n",
    "Now let's load some data. [SOWPODS](https://www.wordgamedictionary.com/sowpods/) is the Official Scrabble Dictionary. It contains every valid word for competitive Scrabble tournaments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/SOWPODS.txt MapPartitionsRDD[56] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd = sc.textFile('./data/SOWPODS.txt')\n",
    "my_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You should see something like : `./data/SOWPODS.txt MapPartitionsRDD[20] at textFile at NativeMethodAccessorImpl.java:0`\n",
    "\n",
    "Because RDDs use [lazy loading](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations), we can't actually see what's inside my_rdd yet.\n",
    "\n",
    "To see contents, we need to execute a command with a data sink: `top`.\n",
    "\n",
    "`top(n)` grabs the first `n` results from a Spark RDD and returns them as a regular python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'zzzs',\n",
       " u'zzz',\n",
       " u'zyzzyvas',\n",
       " u'zyzzyva',\n",
       " u'zythums',\n",
       " u'zythum',\n",
       " u'zymurgy',\n",
       " u'zymurgies',\n",
       " u'zymotics',\n",
       " u'zymotically',\n",
       " u'zymotic',\n",
       " u'zymotechnics',\n",
       " u'zymotechnical',\n",
       " u'zymotechnic',\n",
       " u'zymosis',\n",
       " u'zymosimeters',\n",
       " u'zymosimeter',\n",
       " u'zymoses',\n",
       " u'zymosans',\n",
       " u'zymosan']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd.top(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Basic RDD operations ##\n",
    "\n",
    "Let's start applying some basic [RDD operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations) to the list: filter and collect.\n",
    "\n",
    "Note the syntax change. Those line-ending backslashes are a bit of a a pain, but the stacked syntax is very good for readability. Readability matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_words = my_rdd \\\n",
    "    .filter(lambda word: len(word)>0) \\\n",
    "    .filter(lambda word: word[0] == 'x') \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Like `top`, `collect` returns a regular python list. Unlike `top`, `collect` returns *all* the results.\n",
    "\n",
    "We can use normal python list syntax to inspect the x_words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'xantham',\n",
       " u'xanthams',\n",
       " u'xanthan',\n",
       " u'xanthans',\n",
       " u'xanthate',\n",
       " u'xanthates',\n",
       " u'xanthation',\n",
       " u'xanthations',\n",
       " u'xanthein',\n",
       " u'xantheins',\n",
       " u'xanthene',\n",
       " u'xanthenes',\n",
       " u'xanthic',\n",
       " u'xanthin',\n",
       " u'xanthine',\n",
       " u'xanthines',\n",
       " u'xanthins',\n",
       " u'xanthism',\n",
       " u'xanthisms',\n",
       " u'xanthochroia']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hello, map reduce! ##\n",
    "\n",
    "Now let's run the classic \"hello world\" script for distributed processing: wordcount.\n",
    "\n",
    "In the scrabble dictionary, wordCount is boring: every word occurs once. So instead, let's do letterCount.\n",
    "\n",
    "Let's start by using just `x_words`. We can use `parallelize` to convert a python list into a Spark RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_word_rdd = sc.parallelize(x_words)\n",
    "x_word_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here's letter count, in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', 213),\n",
       " (u'c', 63),\n",
       " (u'e', 270),\n",
       " (u'g', 59),\n",
       " (u'i', 194),\n",
       " (u'm', 82),\n",
       " (u'o', 306),\n",
       " (u's', 214),\n",
       " (u'u', 39),\n",
       " (u'y', 127),\n",
       " (u'b', 18),\n",
       " (u'd', 31),\n",
       " (u'f', 4),\n",
       " (u'h', 169),\n",
       " (u'l', 146),\n",
       " (u'n', 201),\n",
       " (u'p', 122),\n",
       " (u'r', 168),\n",
       " (u't', 141),\n",
       " (u'x', 309)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_count_list = x_word_rdd \\\n",
    "    .flatMap(list) \\\n",
    "    .map(lambda x: (x,1)) \\\n",
    "    .reduceByKey(lambda x, y: x+y) \\\n",
    "    .collect()\n",
    "\n",
    "letter_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114b67690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph_letter_counts(letter_counts):\n",
    "    x, y = zip(*letter_counts)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticklabels(x)\n",
    "    ax.set_xticks(range(len(x)))\n",
    "    plt.bar(range(len(x)), y)\n",
    "    plt.title(\"Letter counts\")\n",
    "    plt.xlabel(\"Letter\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "graph_letter_counts(letter_count_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Nice, but it'd be better sorted. Let's sort it two ways. First, alphabetically. Second, by total count.\n",
    "\n",
    "Let's make our code DRY by factoring out the counting part of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "letter_counts = x_word_rdd \\\n",
    "    .flatMap(list) \\\n",
    "    .map(lambda x: (x,1)) \\\n",
    "    .reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now sort by letter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF9lJREFUeJzt3Xu4XXV95/H3B0S8gHI7psgt2KYzgo6hE1FROxS1Uq0P\n2AoTtYpTa3SkXkaxQ6x9Sm0zxXobH6deglKxohgv1CjeMKIUVDBghCSApJIU8nA5oihIRRO+88de\nkU1YOWfvk6yzT3Ler+fZz1nrt9Zvre9O9tmfs+6pKiRJ2tpuoy5AkjQzGRCSpFYGhCSplQEhSWpl\nQEiSWhkQkqRWBoQkqZUBoZ1akvVJnrm9/ZLMTVJJHrRjK5xeu8r70MxgQEg7gF/I2hUZENplJfnD\nJKuS3JHkW0n+S9P+z8ChwOeT3JXkL4CLm253NG1Paeb90yTXJPlJkq8kOaxv+ZXk1CTXA9dvo4an\nNeu+I8mNSV7WtD8yyUeTjCfZkOQtSXZrpp2R5GN9y7jfVkGSbyT52ySXJrkzyVeTHNDM/oD3keS3\nknwzyU+T/CjJJ3fMv7B2dQaEdklJjgLOBl4J7A98EFieZM+qegnw78DzqmqvqvoH4Hebrvs0bd9O\ncgLwZuCPgDHgX4FPbLWqE4EnAUe01HAY8CXgvU3/+cCqZvJ7gUcCjwH+G/BS4H8M8RZf1Mz/KODB\nwGlN+wPeB/C3wFeBfYGDm3VLkzIgtKtaBHywqi6rqs1VdQ5wD/DkIZbxKuDvq+qaqtoE/B9gfv9W\nRDP9x1X1Hy39XwR8rao+UVW/qqrbq2pVkt2BhcDiqrqzqtYD7wReMkRt/1RVP2jWu4xe+GzLr4DD\ngEdX1S+q6pIh1qNZzIDQruow4I3Nrp07ktwBHAI8eshlvKev/4+BAAf1zXPjBP0PAf6tpf0AYA9g\nQ1/bhq2WO5lb+obvBvaaYN6/oFf35UnWJPnTIdajWcwDa9pV3Qgsqaol25i+9W2M225rvGUZ506w\nnoluh3wjcHRL+4+476/6tU3bocDGZvjnwMP65v+NCdYxaT1VdQvwCugdEwG+luTiqlo3xHI1C7kF\noV3BHkke0vd6EHAW8KokT0rPw5M8N8neTZ9b6e3/32IcuHertg8Ai5McCb8+sHzSEHWdCzwzyclJ\nHpRk/yTzq2ozvd1CS5Ls3eyyegOw5cD0KuB3kxya5JHA4iHW+YD3keSkJAc3oz+hFyL3DrFMzVIG\nhHYFXwT+o+91RlWtpPdX8/+j96W4DnhZX5+/B97S7D46raruBpYAlzZtT66q84G3Aecl+RmwGviD\nQYuqqn8HngO8kd7uqVXAE5rJr6G3pfBD4BLg4/QOqlNVFwKfBK4CrgC+MMQ6H/A+gCcClyW5C1gO\nvK6qfjjoMjV7xQcGSZLauAUhSWplQEiSWhkQkqRWBoQkqdVOfR3EAQccUHPnzh11GZK0U7niiit+\nVFVjk823UwfE3LlzWbly5ajLkKSdSpINk8/lLiZJ0jYYEJKkVgaEJKmVASFJamVASJJaGRCSpFYG\nhCSplQEhSWplQEiSWu3UV1JL0kwx9/QLhu6z/szndlDJjuMWhCSplVsQ0i5q2L9oZ/pfs5p+nW1B\nNA+PvzzJ95OsSfI3Tft+SS5Mcn3zc9++PouTrEtyXZJnd1WbJGlyXe5iugc4rqqeAMwHjm8eoH46\nsKKq5gErmnGSHAEsBI4Ejgfel2T3DuuTJE2gs4Conrua0T2aVwEnAOc07ecAJzbDJwDnVdU9VXUD\nsA44uqv6JEkT6/QgdZLdk6wCbgMurKrLgDlVdXMzyy3AnGb4IODGvu43NW1bL3NRkpVJVo6Pj3dY\nvSTNbp0GRFVtrqr5wMHA0Uket9X0ordVMcwyl1bVgqpaMDY26QORJElTNC2nuVbVHcBF9I4t3Jrk\nQIDm523NbBuBQ/q6Hdy0SZJGoMuzmMaS7NMMPxR4FnAtsBw4pZntFOBzzfByYGGSPZMcDswDLu+q\nPknSxLq8DuJA4JzmTKTdgGVV9YUk3waWJXk5sAE4GaCq1iRZBqwFNgGnVtXmDuuTJE2gs4CoqquA\no1rabweesY0+S4AlXdUkSRqct9qQJLUyICRJrQwISVIrA0KS1MqAkCS1MiAkSa0MCElSKwNCktTK\ngJAktTIgJEmtDAhJUisDQpLUyoCQJLUyICRJrbp8HoRmoLmnXzDU/OvPfG5HlUia6dyCkCS1MiAk\nSa0MCElSKwNCktTKgJAktTIgJEmtDAhJUisDQpLUyoCQJLXqLCCSHJLkoiRrk6xJ8rqm/YwkG5Os\nal7P6euzOMm6JNcleXZXtUmSJtflrTY2AW+sqiuT7A1ckeTCZtq7q+od/TMnOQJYCBwJPBr4WpLf\nrqrNHdYoSdqGzrYgqurmqrqyGb4TuAY4aIIuJwDnVdU9VXUDsA44uqv6JEkTm5ZjEEnmAkcBlzVN\nr0lyVZKzk+zbtB0E3NjX7SZaAiXJoiQrk6wcHx/vsGpJmt06D4gkewGfAV5fVT8D3g88BpgP3Ay8\nc5jlVdXSqlpQVQvGxsZ2eL2SpJ5OAyLJHvTC4dyq+ixAVd1aVZur6l7gLO7bjbQROKSv+8FNmyRp\nBLo8iynAh4Frqupdfe0H9s32fGB1M7wcWJhkzySHA/OAy7uqT5I0sS7PYnoq8BLg6iSrmrY3Ay9M\nMh8oYD3wSoCqWpNkGbCW3hlQp3oGkySNTmcBUVWXAGmZ9MUJ+iwBlnRVkyRpcF5JLUlqZUBIkloZ\nEJKkVgaEJKmVASFJamVASJJaGRCSpFYGhCSplQEhSWplQEiSWhkQkqRWBoQkqZUBIUlqZUBIkloZ\nEJKkVgaEJKmVASFJamVASJJadflMakka2tzTLxhq/vVnPrejSjSrA8IPoiRtm7uYJEmtDAhJUisD\nQpLUyoCQJLUyICRJrToLiCSHJLkoydoka5K8rmnfL8mFSa5vfu7b12dxknVJrkvy7K5qkyRNrsst\niE3AG6vqCODJwKlJjgBOB1ZU1TxgRTNOM20hcCRwPPC+JLt3WJ8kaQKdXQdRVTcDNzfDdya5BjgI\nOAE4tpntHOAbwP9u2s+rqnuAG5KsA44Gvt1VjdJM57U6GqVpOQaRZC5wFHAZMKcJD4BbgDnN8EHA\njX3dbmratl7WoiQrk6wcHx/vrGZJmu06D4gkewGfAV5fVT/rn1ZVBdQwy6uqpVW1oKoWjI2N7cBK\nJUn9Og2IJHvQC4dzq+qzTfOtSQ5sph8I3Na0bwQO6et+cNMmSRqBLs9iCvBh4JqqelffpOXAKc3w\nKcDn+toXJtkzyeHAPODyruqTJE2sy5v1PRV4CXB1klVN25uBM4FlSV4ObABOBqiqNUmWAWvpnQF1\nalVt7rA+zSIe7JWG1+VZTJcA2cbkZ2yjzxJgSVc1SZIG55XUkqRWBoQkqZUBIUlqZUBIkloZEJKk\nVgaEJKmVASFJajVQQCR56iBtkqRdx6BbEO8dsE2StIuY8ErqJE8BjgHGkryhb9IjAB/mI0m7sMlu\ntfFgYK9mvr372n8GvKCroiRJozdhQFTVN4FvJvlIVW2YppokSTPAoDfr2zPJUmBuf5+qOq6LoiRJ\nozdoQHwK+ADwIcBbcEvSLDBoQGyqqvd3WokkaUYZNCA+n+TVwPnAPVsaq+rHnVQlbcUH/uxc/P/a\nNQwaEFseEfqmvrYCHrNjy5EkzRQDBURVHd51IZKkmWWggEjy0rb2qvroji1HkjRTDLqL6Yl9ww+h\n90zpKwEDQpJ2UYPuYnpN/3iSfYDzOqlIkjQjTPV23z8HPC4hSbuwQY9BfJ7eWUvQu0nfY4FlXRUl\nSRq9QY9BvKNveBOwoapu6qAeSdIMMdAupuamfdfSu6PrvsAvJ+uT5OwktyVZ3dd2RpKNSVY1r+f0\nTVucZF2S65I8e/i3IknakQZ9otzJwOXAScDJwGVJJrvd90eA41va311V85vXF5vlHwEsBI5s+rwv\nic+bkKQRGnQX018CT6yq2wCSjAFfAz69rQ5VdXGSuQMu/wTgvKq6B7ghyTrgaODbA/aXJO1gg57F\ntNuWcGjcPkTfrb0myVXNLqh9m7aDgBv75rmpaXuAJIuSrEyycnx8fIolSJImM+iX/JeTfCXJy5K8\nDLgA+OIU1vd+evdvmg/cDLxz2AVU1dKqWlBVC8bGxqZQgiRpEJM9k/q3gDlV9aYkfwQ8rZn0beDc\nYVdWVbf2Lfss4AvN6EbgkL5ZD27aJEkjMtkxiP8LLAaoqs8CnwVI8vhm2vOGWVmSA6vq5mb0+cCW\nM5yWAx9P8i7g0cA8egfFpZHz1tWarSYLiDlVdfXWjVV19WQHoJN8AjgWOCDJTcBfA8cmmU/vorv1\nwCub5a1JsgxYS+86i1OryifXSdIITRYQ+0ww7aETdayqF7Y0f3iC+ZcASyapR5I0TSYLiJVJXlFV\nZ/U3Jvkz4IruypKk4Q27OxDcJTiRyQLi9cD5SV7MfYGwAHgwvWMIkqRd1IQB0Zx1dEyS3wMe1zRf\nUFVf77wySdJIDfo8iIuAizquRZI0g0z1amhJ0i7OgJAktTIgJEmtDAhJUisDQpLUyoCQJLUa9IFB\nkqQOzcSbQroFIUlqZUBIklq5i0nSA3jTO4FbEJKkbTAgJEmtDAhJUisDQpLUyoCQJLUyICRJrQwI\nSVIrA0KS1MqAkCS1MiAkSa0MCElSq84CIsnZSW5Lsrqvbb8kFya5vvm5b9+0xUnWJbkuybO7qkuS\nNJgutyA+Ahy/VdvpwIqqmgesaMZJcgSwEDiy6fO+JLt3WJskaRKdBURVXQz8eKvmE4BzmuFzgBP7\n2s+rqnuq6gZgHXB0V7VJkiY33ccg5lTVzc3wLcCcZvgg4Ma++W5q2h4gyaIkK5OsHB8f765SSZrl\nRnaQuqoKqCn0W1pVC6pqwdjYWAeVSZJg+gPi1iQHAjQ/b2vaNwKH9M13cNMmSRqR6Q6I5cApzfAp\nwOf62hcm2TPJ4cA84PJprk2S1KezR44m+QRwLHBAkpuAvwbOBJYleTmwATgZoKrWJFkGrAU2AadW\n1eauapMkTa6zgKiqF25j0jO2Mf8SYElX9UiShuOV1JKkVgaEJKmVASFJamVASJJaGRCSpFYGhCSp\nVWenuUqCuadfMHSf9Wc+t4NKpOG5BSFJamVASJJaGRCSpFYGhCSplQEhSWplQEiSWhkQkqRWBoQk\nqZUBIUlqZUBIkloZEJKkVgaEJKmVN+sbAW/gJmln4BaEJKmVWxCaNsNuObnVJI2WWxCSpFYGhCSp\n1Uh2MSVZD9wJbAY2VdWCJPsBnwTmAuuBk6vqJ6OoT5I02i2I36uq+VW1oBk/HVhRVfOAFc24JGlE\nZtIuphOAc5rhc4ATR1iLJM16owqIAr6W5Ioki5q2OVV1czN8CzCnrWOSRUlWJlk5Pj4+HbVK0qw0\nqtNcn1ZVG5M8CrgwybX9E6uqklRbx6paCiwFWLBgQes8kqTtN5ItiKra2Py8DTgfOBq4NcmBAM3P\n20ZRmySpZ9oDIsnDk+y9ZRj4fWA1sBw4pZntFOBz012bJOk+o9jFNAc4P8mW9X+8qr6c5LvAsiQv\nBzYAJ4+gNklSY9oDoqp+CDyhpf124BnTXY8kqd1MOs1VkjSDGBCSpFbezVWSGt5x+P7cgpAktTIg\nJEmtDAhJUisDQpLUyoPUOyEPpEmaDm5BSJJaGRCSpFYGhCSplQEhSWplQEiSWhkQkqRWBoQkqZUB\nIUlq5YVyU+TFapJ2dW5BSJJaGRCSpFYGhCSplQEhSWplQEiSWhkQkqRWBoQkqZUBIUlqNeMulEty\nPPAeYHfgQ1V15ohLUmPYiwPBCwSlndmM2oJIsjvwj8AfAEcAL0xyxGirkqTZaUYFBHA0sK6qflhV\nvwTOA04YcU2SNCulqkZdw68leQFwfFX9WTP+EuBJVfXnffMsAhY1o/8JuK6DUg4AfrST9Z2t67bu\n2bNu695xDquqsclmmnHHICZTVUuBpV2uI8nKqlqwM/Wdreu27tmzbuuefjNtF9NG4JC+8YObNknS\nNJtpAfFdYF6Sw5M8GFgILB9xTZI0K82oXUxVtSnJnwNfoXea69lVtWYEpWzPLqxR9Z2t67bu2bNu\n655mM+ogtSRp5phpu5gkSTOEASFJamVA7CBJ5iZZPQPqOCPJadO8ztcmuSbJudO4zu3+907yre3s\nf9f29Nf0SLJPklePuo6dkQGhHeHVwLOq6sWjLmQYVXXMqGuYrdIzXd8/+9D7jGpIBkSfJP+S5Iok\na5ortof1oCTnNn9NfzrJw4ZY90uTXJXk+0n+eZiVJvnLJD9Icgm9q8uHkuRPklyeZFWSDzb3xBq0\n7weAxwBfSvK/hlzvXyW5LsklST4xhS2f3ZOc1fx/fTXJQ4dc/7RtATRbPNcm+Ujzf3VukmcmuTTJ\n9UmOHnAZ12zne35DktXN6/VTqH9Kn+++ZVyX5KPAau5/zdNE/R6e5ILmd2N1kv8+zHqBM4HfbD7f\nb59Czav7xk9LcsaAfZ/Y/E4/pHkPa5I8bsC+b+3//0myJMnrhql9h6gqX80L2K/5+VB6H+D9h+g7\nFyjgqc342cBpA/Y9EvgBcEB/HQP2/a/A1cDDgEcA6wZdb9P/scDngT2a8fcBLx3y3239ltqH6PNE\nYBXwEGBv4Poh654LbALmN+PLgD8Zsoa7tvPzMnD/vnofT+8Psyuaz0jo3W/sX7p+z32flYcDewFr\ngKOGqH9Kn++tlnEv8OQh+/0xcFbf+COnsN7VU/w/vl9f4DTgjCH6/x3wDno3IV085HqvbIZ3A/5t\nmO+jHfVyC+L+Xpvk+8B36P11M2/I/jdW1aXN8MeApw3Y7zjgU1X1I4Cq+vEQ63w6cH5V3V1VP2P4\nCwufQe+L47tJVjXjjxlyGVPxVOBzVfWLqrqTXkgN64aqWtUMX0Hvl2omu6Gqrq6qe+l9Oa+o3jfA\n1Qxe+/a856fR+6z8vKruAj5L7/MzqKl+vvttqKrvDNnnauBZSd6W5OlV9dMprHdU3go8C1gA/MOg\nnapqPXB7kqOA3we+V1W3d1LhBGbUhXKjlORY4JnAU6rq7iTfoPfX7TC2vqhkZ7jIJMA5VbV41IVM\nwT19w5vpbfnNZP313ts3fi+D/y6O8j3viM/3z4deadUPkvwO8Bzg75KsqKq3TmHdU7GJ+++KH/Y7\nYX96W2t7NH2Hef8fAl4G/Aa9LbZp5xbEfR4J/KQJh/8MPHkKyzg0yVOa4RcBlwzY7+vASUn2B0iy\n3xDrvBg4MclDk+wNPG+IvgArgBckedSWdSc5bMhlTMWlwPOa/bN7AX84Deuc7f6V3mflYUkeDjy/\naRvUVD/f2yXJo4G7q+pjwNuB3xlyEXfS2405FbcCj0qyf5I9Gf5z+kHgr4BzgbcN2fd84Hh6u2O/\nMmTfHcItiPt8GXhVkmvo3UJ82M1gmn6nJjkbWAu8f5BOVbUmyRLgm0k2A9+j95fDIH2vTPJJ4PvA\nbfTuZzWwqlqb5C3AV5uzSn4FnApsGGY5w6qq7yZZDlxF75fwamBn2nWw02k+Kx8BLm+aPlRV3xti\nEVP6fO8AjwfenuReep/P/zlM56q6vTkZYDXwpap60xB9f5XkrfT+zTYC1w7aN8lLgV9V1cebEz++\nleS4qvr6gOv+ZZKLgDuqavOg692RvNWGRibJXlV1V3M2zMXAoqq6ctR16YGSzAW+UFUDnYWj7df8\nwXYlcFJVXT+KGtzFpFFa2hwYvxL4jOEg9aT3qOV19E5kGEk4gFsQkqRtcAtCktTKgJAktTIgJEmt\nDAhpEsPcsynJsUmO6Rs/sTngKO10DAhpxzoW6L9L7InAUAGRxOuTNCN4FpM0iSR3VdVeW7WNAR8A\nDm2aXk/vQqrv0LsFxjjwOnpXw/60ef1xM+8/AmPA3cArqura5gK2XwBHAZdW1Ru6fE/SIPxLRZqa\n9wDvrqpLkhwKfKWqHtvc/vyuqnoHQHO1+Beq6tPN+ArgVVV1fZIn0bt77nHNMg8GjhnVVbPS1gwI\naWqeCRyRZMv4I5p7Sm1TM/0Y4FN9/fbsm+VThoNmEgNCmprd6D3X4Bf9jX1f/Nvqc0dVzd/G9KHv\ndCp1yYPU0tR8FXjNlpEkW770t75z6K/Hm+d13JDkpKZPkjxhesqVhmdASJN7WJKb+l5vAF4LLGge\nKbkWeFUz7+eB5zePt3w6cB7wpiTfS/KbwIuBlzcPplpD72ly0ozkWUySpFZuQUiSWhkQkqRWBoQk\nqZUBIUlqZUBIkloZEJKkVgaEJKnV/wdS7ofRyaK42AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1149c8e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "letter_count_list = letter_counts \\\n",
    "    .sortByKey() \\\n",
    "    .collect()\n",
    "\n",
    "graph_letter_counts(letter_count_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now sort by value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116210dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "letter_count_list = letter_counts \\\n",
    "    .map(lambda (a,b): (b, a)) \\\n",
    "    .sortByKey(ascending=False) \\\n",
    "    .map(lambda (a,b): (b, a)) \\\n",
    "    .collect()\n",
    "\n",
    "graph_letter_counts(letter_count_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Blending python and RDDs ##\n",
    "\n",
    "One of the huge advantages of pyspark is that you get the strengths of *both* languages.\n",
    "\n",
    "Let's blend python and spark to identify palindromes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def is_palindrome(my_str):\n",
    "    return my_str == my_str[::-1]\n",
    "\n",
    "assert is_palindrome(\"\")\n",
    "assert is_palindrome(\"a\")\n",
    "assert is_palindrome(\"AA\")\n",
    "assert is_palindrome(\"ABA\")\n",
    "assert is_palindrome(\"amanaplanacanalpanama\")\n",
    "assert is_palindrome(\"AB\") == False\n",
    "assert is_palindrome(\"ABC\") == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we plug our native python function into an RDD-based computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'aa',\n",
       " u'aba',\n",
       " u'abba',\n",
       " u'acca',\n",
       " u'aga',\n",
       " u'aha',\n",
       " u'aia',\n",
       " u'aka',\n",
       " u'ala',\n",
       " u'alula',\n",
       " u'ama',\n",
       " u'ana',\n",
       " u'anana',\n",
       " u'anna',\n",
       " u'araara',\n",
       " u'ataata',\n",
       " u'aua',\n",
       " u'ava',\n",
       " u'awa',\n",
       " u'bib',\n",
       " u'bob',\n",
       " u'boob',\n",
       " u'bub',\n",
       " u'civic',\n",
       " u'dad',\n",
       " u'deed',\n",
       " u'degged',\n",
       " u'deified',\n",
       " u'deked',\n",
       " u'deled',\n",
       " u'denned',\n",
       " u'dered',\n",
       " u'dewed',\n",
       " u'did',\n",
       " u'dod',\n",
       " u'dud',\n",
       " u'ecce',\n",
       " u'ee',\n",
       " u'eke',\n",
       " u'eme',\n",
       " u'ene',\n",
       " u'ere',\n",
       " u'esse',\n",
       " u'eve',\n",
       " u'ewe',\n",
       " u'eye',\n",
       " u'gag',\n",
       " u'gig',\n",
       " u'goog',\n",
       " u'hadedah',\n",
       " u'hah',\n",
       " u'hajjah',\n",
       " u'halalah',\n",
       " u'hallah',\n",
       " u'heh',\n",
       " u'hoh',\n",
       " u'huh',\n",
       " u'iwi',\n",
       " u'kaiak',\n",
       " u'kak',\n",
       " u'kayak',\n",
       " u'keek',\n",
       " u'kook',\n",
       " u'lemel',\n",
       " u'level',\n",
       " u'madam',\n",
       " u'malam',\n",
       " u'mallam',\n",
       " u'mam',\n",
       " u'marram',\n",
       " u'mem',\n",
       " u'mim',\n",
       " u'minim',\n",
       " u'mm',\n",
       " u'mom',\n",
       " u'mum',\n",
       " u'naan',\n",
       " u'nan',\n",
       " u'non',\n",
       " u'noon',\n",
       " u'nun',\n",
       " u'obo',\n",
       " u'oho',\n",
       " u'ono',\n",
       " u'oo',\n",
       " u'oppo',\n",
       " u'otto',\n",
       " u'oxo',\n",
       " u'pap',\n",
       " u'peep',\n",
       " u'pep',\n",
       " u'pip',\n",
       " u'poop',\n",
       " u'pop',\n",
       " u'pullup',\n",
       " u'pup',\n",
       " u'radar',\n",
       " u'redder',\n",
       " u'refer',\n",
       " u'reifier',\n",
       " u'repaper',\n",
       " u'reviver',\n",
       " u'rotator',\n",
       " u'rotavator',\n",
       " u'rotor',\n",
       " u'sagas',\n",
       " u'samas',\n",
       " u'sedes',\n",
       " u'sees',\n",
       " u'seities',\n",
       " u'seles',\n",
       " u'selles',\n",
       " u'sememes',\n",
       " u'semes',\n",
       " u'seres',\n",
       " u'serres',\n",
       " u'sesses',\n",
       " u'sexes',\n",
       " u'shahs',\n",
       " u'simis',\n",
       " u'siris',\n",
       " u'sis',\n",
       " u'solos',\n",
       " u'sos',\n",
       " u'stats',\n",
       " u'stets',\n",
       " u'stots',\n",
       " u'succus',\n",
       " u'sulus',\n",
       " u'sus',\n",
       " u'susus',\n",
       " u'tallat',\n",
       " u'tat',\n",
       " u'tenet',\n",
       " u'terret',\n",
       " u'tet',\n",
       " u'tirrit',\n",
       " u'tit',\n",
       " u'toot',\n",
       " u'torot',\n",
       " u'tot',\n",
       " u'tut',\n",
       " u'ulu',\n",
       " u'umu',\n",
       " u'utu',\n",
       " u'vav',\n",
       " u'waw',\n",
       " u'wow',\n",
       " u'yay',\n",
       " u'ziz',\n",
       " u'zuz',\n",
       " u'zzz']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palindromes = my_rdd \\\n",
    "    .filter(lambda x: len(x)>0) \\\n",
    "    .filter(is_palindrome) \\\n",
    "    \n",
    "palindromes.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just for fun, let's count letters again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', 91),\n",
       " (u'b', 12),\n",
       " (u'c', 8),\n",
       " (u'd', 31),\n",
       " (u'e', 87),\n",
       " (u'f', 3),\n",
       " (u'g', 10),\n",
       " (u'h', 20),\n",
       " (u'i', 29),\n",
       " (u'j', 2),\n",
       " (u'k', 13),\n",
       " (u'l', 25),\n",
       " (u'm', 31),\n",
       " (u'n', 21),\n",
       " (u'o', 46),\n",
       " (u'p', 20),\n",
       " (u'r', 33),\n",
       " (u's', 57),\n",
       " (u't', 39),\n",
       " (u'u', 25),\n",
       " (u'v', 9),\n",
       " (u'w', 8),\n",
       " (u'x', 2),\n",
       " (u'y', 4),\n",
       " (u'z', 7)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palindrome_letter_counts = palindromes \\\n",
    "    .flatMap(list) \\\n",
    "    .map(lambda x: (x,1)) \\\n",
    "    .reduceByKey(lambda x, y: x+y)\n",
    "    \n",
    "palindrome_letter_counts \\\n",
    "    .sortByKey() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## A grab-bag of operators and actions ##\n",
    "\n",
    "Here are some other useful pyspark actions you can use. Most of these are `actions`, meaning that they complete an RDD calculation chain and return some other kind of object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'a', 91)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palindrome_letter_counts.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', 91),\n",
       " (u'c', 8),\n",
       " (u'e', 87),\n",
       " (u'g', 10),\n",
       " (u'i', 29),\n",
       " (u'k', 13),\n",
       " (u'm', 31),\n",
       " (u'o', 46),\n",
       " (u's', 57),\n",
       " (u'u', 25)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palindrome_letter_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'j', 2),\n",
       " (u'x', 2),\n",
       " (u'f', 3),\n",
       " (u'y', 4),\n",
       " (u'z', 7),\n",
       " (u'c', 8),\n",
       " (u'w', 8),\n",
       " (u'v', 9),\n",
       " (u'g', 10),\n",
       " (u'b', 12)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palindrome_letter_counts.takeOrdered(10, lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', 91),\n",
       " (u'e', 87),\n",
       " (u's', 57),\n",
       " (u'o', 46),\n",
       " (u't', 39),\n",
       " (u'r', 33),\n",
       " (u'm', 31),\n",
       " (u'd', 31),\n",
       " (u'i', 29),\n",
       " (u'u', 25)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palindrome_letter_counts.takeOrdered(10, lambda x: -1*x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', 91),\n",
       " (u'b', 12),\n",
       " (u'c', 8),\n",
       " (u'd', 31),\n",
       " (u'e', 87),\n",
       " (u'f', 3),\n",
       " (u'g', 10),\n",
       " (u'h', 20),\n",
       " (u'i', 29),\n",
       " (u'j', 2)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palindrome_letter_counts.takeOrdered(10, lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267759"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5507\n"
     ]
    }
   ],
   "source": [
    "sampled_words = my_rdd \\\n",
    "    .sample(False, .02) \\\n",
    "    .collect()\n",
    "\n",
    "print len(sampled_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 1 - Filters and actions ##\n",
    "\n",
    "1. Get a sample of 5% of the SOWPODS words.\n",
    "2. Find the first word that start with J.\n",
    "3. Find the last 10 words that start with J.\n",
    "4. How many words end with the letter E?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13232\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 106.0 failed 1 times, most recent failure: Lost task 0.0 in stage 106.0 (TID 151, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 2407, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 2407, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 1290, in <lambda>\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File \"/Users/ceoxxx/anaconda/envs/py2/lib/python2.7/heapq.py\", line 432, in nsmallest\n    result = _nsmallest(n, it)\n  File \"<ipython-input-65-d7804f1f569b>\", line 7, in <lambda>\nTypeError: bad operand type for unary -: 'unicode'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 2407, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 2407, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 1290, in <lambda>\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File \"/Users/ceoxxx/anaconda/envs/py2/lib/python2.7/heapq.py\", line 432, in nsmallest\n    result = _nsmallest(n, it)\n  File \"<ipython-input-65-d7804f1f569b>\", line 7, in <lambda>\nTypeError: bad operand type for unary -: 'unicode'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-d7804f1f569b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_rdd\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'J'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'j'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mtakeOrdered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtakeOrdered\u001b[0;34m(self, num, key)\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsmallest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mheapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsmallest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 106.0 failed 1 times, most recent failure: Lost task 0.0 in stage 106.0 (TID 151, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 2407, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 2407, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 1290, in <lambda>\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File \"/Users/ceoxxx/anaconda/envs/py2/lib/python2.7/heapq.py\", line 432, in nsmallest\n    result = _nsmallest(n, it)\n  File \"<ipython-input-65-d7804f1f569b>\", line 7, in <lambda>\nTypeError: bad operand type for unary -: 'unicode'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 2407, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 2407, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.py\", line 1290, in <lambda>\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File \"/Users/ceoxxx/anaconda/envs/py2/lib/python2.7/heapq.py\", line 432, in nsmallest\n    result = _nsmallest(n, it)\n  File \"<ipython-input-65-d7804f1f569b>\", line 7, in <lambda>\nTypeError: bad operand type for unary -: 'unicode'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "sowpods_rdd = sc.textFile('./data/SOWPODS.txt')\n",
    "\n",
    "sampled_words = my_rdd \\\n",
    "    .sample(False, .05) \\\n",
    "    .collect()\n",
    "\n",
    "print len(sampled_words)\n",
    "\n",
    "x_words = my_rdd \\\n",
    "    .filter(lambda word: len(word)>0) \\\n",
    "    .filter(lambda x: x[0] in ['J','j']) \\\n",
    "    .takeOrdered(10, key = lambda x: -x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(x_words)\n",
    "\n",
    "\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 2 - Counting and cleaning ##\n",
    "\n",
    "What is the distribution of word lengths in SOWPODS? That is, how many 2-letter, 3-letter, 4-letter, etc. words does the dictionary contain?\n",
    "\n",
    "(Note: You'll have to do a little bit of data cleaning to make this work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'SOWPODS (Europe Scrabble Word List)',\n",
       " u'',\n",
       " u'Did you know this is a European word list? You can find the US version here http://www.wordgamedictionary.com/twl06/download/twl06.txt',\n",
       " u'',\n",
       " u'You can also find an extensive and larger English word list here http://www.wordgamedictionary.com/english-word-list/download/english.txt',\n",
       " u'',\n",
       " u'aa',\n",
       " u'aah',\n",
       " u'aahed',\n",
       " u'aahing',\n",
       " u'aahs',\n",
       " u'aal',\n",
       " u'aalii',\n",
       " u'aaliis',\n",
       " u'aals',\n",
       " u'aardvark',\n",
       " u'aardvarks',\n",
       " u'aardwolf',\n",
       " u'aardwolves',\n",
       " u'aargh',\n",
       " u'aarrgh',\n",
       " u'aarrghh',\n",
       " u'aarti',\n",
       " u'aartis',\n",
       " u'aas',\n",
       " u'aasvogel',\n",
       " u'aasvogels',\n",
       " u'ab',\n",
       " u'aba',\n",
       " u'abac',\n",
       " u'abaca',\n",
       " u'abacas',\n",
       " u'abaci',\n",
       " u'aback',\n",
       " u'abacs',\n",
       " u'abacterial',\n",
       " u'abactinal',\n",
       " u'abactinally',\n",
       " u'abactor',\n",
       " u'abactors',\n",
       " u'abacus',\n",
       " u'abacuses',\n",
       " u'abaft',\n",
       " u'abaka',\n",
       " u'abakas',\n",
       " u'abalone',\n",
       " u'abalones',\n",
       " u'abamp',\n",
       " u'abampere',\n",
       " u'abamperes',\n",
       " u'abamps',\n",
       " u'aband',\n",
       " u'abanded',\n",
       " u'abanding',\n",
       " u'abandon',\n",
       " u'abandoned',\n",
       " u'abandonedly',\n",
       " u'abandonee',\n",
       " u'abandonees',\n",
       " u'abandoner',\n",
       " u'abandoners',\n",
       " u'abandoning',\n",
       " u'abandonment',\n",
       " u'abandonments',\n",
       " u'abandons',\n",
       " u'abandonware',\n",
       " u'abandonwares',\n",
       " u'abands',\n",
       " u'abapical',\n",
       " u'abas',\n",
       " u'abase',\n",
       " u'abased',\n",
       " u'abasedly',\n",
       " u'abasement',\n",
       " u'abasements',\n",
       " u'abaser',\n",
       " u'abasers',\n",
       " u'abases',\n",
       " u'abash',\n",
       " u'abashed',\n",
       " u'abashedly',\n",
       " u'abashes',\n",
       " u'abashing',\n",
       " u'abashless',\n",
       " u'abashment',\n",
       " u'abashments',\n",
       " u'abasia',\n",
       " u'abasias',\n",
       " u'abasing',\n",
       " u'abask',\n",
       " u'abatable',\n",
       " u'abate',\n",
       " u'abated',\n",
       " u'abatement',\n",
       " u'abatements',\n",
       " u'abater',\n",
       " u'abaters',\n",
       " u'abates',\n",
       " u'abating',\n",
       " u'abatis',\n",
       " u'abatises',\n",
       " u'abator',\n",
       " u'abators',\n",
       " u'abattis',\n",
       " u'abattises',\n",
       " u'abattoir',\n",
       " u'abattoirs',\n",
       " u'abattu',\n",
       " u'abature',\n",
       " u'abatures',\n",
       " u'abaxial',\n",
       " u'abaxile',\n",
       " u'abaya',\n",
       " u'abayas',\n",
       " u'abb',\n",
       " u'abba',\n",
       " u'abbacies',\n",
       " u'abbacy',\n",
       " u'abbas',\n",
       " u'abbatial',\n",
       " u'abbe',\n",
       " u'abbed',\n",
       " u'abbes',\n",
       " u'abbess',\n",
       " u'abbesses',\n",
       " u'abbey',\n",
       " u'abbeys',\n",
       " u'abbot',\n",
       " u'abbotcies',\n",
       " u'abbotcy',\n",
       " u'abbots',\n",
       " u'abbotship',\n",
       " u'abbotships',\n",
       " u'abbreviate',\n",
       " u'abbreviated',\n",
       " u'abbreviates',\n",
       " u'abbreviating',\n",
       " u'abbreviation',\n",
       " u'abbreviations',\n",
       " u'abbreviator',\n",
       " u'abbreviators',\n",
       " u'abbreviatory',\n",
       " u'abbreviature',\n",
       " u'abbreviatures',\n",
       " u'abbs',\n",
       " u'abcee',\n",
       " u'abcees',\n",
       " u'abcoulomb',\n",
       " u'abcoulombs',\n",
       " u'abdabs',\n",
       " u'abdicable',\n",
       " u'abdicant',\n",
       " u'abdicate',\n",
       " u'abdicated',\n",
       " u'abdicates',\n",
       " u'abdicating',\n",
       " u'abdication',\n",
       " u'abdications',\n",
       " u'abdicative',\n",
       " u'abdicator',\n",
       " u'abdicators',\n",
       " u'abdomen',\n",
       " u'abdomens',\n",
       " u'abdomina',\n",
       " u'abdominal',\n",
       " u'abdominally',\n",
       " u'abdominals',\n",
       " u'abdominoplasty',\n",
       " u'abdominous',\n",
       " u'abduce',\n",
       " u'abduced',\n",
       " u'abducens',\n",
       " u'abducent',\n",
       " u'abducentes',\n",
       " u'abduces',\n",
       " u'abducing',\n",
       " u'abduct',\n",
       " u'abducted',\n",
       " u'abductee',\n",
       " u'abductees',\n",
       " u'abducting',\n",
       " u'abduction',\n",
       " u'abductions',\n",
       " u'abductor',\n",
       " u'abductores',\n",
       " u'abductors',\n",
       " u'abducts',\n",
       " u'abeam',\n",
       " u'abear',\n",
       " u'abearing',\n",
       " u'abears',\n",
       " u'abecedarian',\n",
       " u'abecedarians',\n",
       " u'abed',\n",
       " u'abegging',\n",
       " u'abeigh',\n",
       " u'abele',\n",
       " u'abeles',\n",
       " u'abelia',\n",
       " u'abelian',\n",
       " u'abelias',\n",
       " u'abelmosk',\n",
       " u'abelmosks',\n",
       " u'aberdevine',\n",
       " u'aberdevines',\n",
       " u'abernethies',\n",
       " u'abernethy',\n",
       " u'aberrance',\n",
       " u'aberrances',\n",
       " u'aberrancies',\n",
       " u'aberrancy',\n",
       " u'aberrant',\n",
       " u'aberrantly',\n",
       " u'aberrants',\n",
       " u'aberrate',\n",
       " u'aberrated',\n",
       " u'aberrates',\n",
       " u'aberrating',\n",
       " u'aberration',\n",
       " u'aberrational',\n",
       " u'aberrations',\n",
       " u'abessive',\n",
       " u'abessives',\n",
       " u'abet',\n",
       " u'abetment',\n",
       " u'abetments',\n",
       " u'abets',\n",
       " u'abettal',\n",
       " u'abettals',\n",
       " u'abetted',\n",
       " u'abetter',\n",
       " u'abetters',\n",
       " u'abetting',\n",
       " u'abettor',\n",
       " u'abettors',\n",
       " u'abeyance',\n",
       " u'abeyances',\n",
       " u'abeyancies',\n",
       " u'abeyancy',\n",
       " u'abeyant',\n",
       " u'abfarad',\n",
       " u'abfarads',\n",
       " u'abhenries',\n",
       " u'abhenry',\n",
       " u'abhenrys',\n",
       " u'abhominable',\n",
       " u'abhor',\n",
       " u'abhorred',\n",
       " u'abhorrence',\n",
       " u'abhorrences',\n",
       " u'abhorrencies',\n",
       " u'abhorrency',\n",
       " u'abhorrent',\n",
       " u'abhorrently',\n",
       " u'abhorrer',\n",
       " u'abhorrers',\n",
       " u'abhorring',\n",
       " u'abhorrings',\n",
       " u'abhors',\n",
       " u'abid',\n",
       " u'abidance',\n",
       " u'abidances',\n",
       " u'abidden',\n",
       " u'abide',\n",
       " u'abided',\n",
       " u'abider',\n",
       " u'abiders',\n",
       " u'abides',\n",
       " u'abiding',\n",
       " u'abidingly',\n",
       " u'abidings',\n",
       " u'abies',\n",
       " u'abietic',\n",
       " u'abigail',\n",
       " u'abigails',\n",
       " u'abilities',\n",
       " u'ability',\n",
       " u'abiogeneses',\n",
       " u'abiogenesis',\n",
       " u'abiogenetic',\n",
       " u'abiogenetically',\n",
       " u'abiogenic',\n",
       " u'abiogenically',\n",
       " u'abiogenist',\n",
       " u'abiogenists',\n",
       " u'abiological',\n",
       " u'abioses',\n",
       " u'abiosis',\n",
       " u'abiotic',\n",
       " u'abiotically',\n",
       " u'abiotrophic',\n",
       " u'abiotrophies',\n",
       " u'abiotrophy',\n",
       " u'abirritant',\n",
       " u'abirritants',\n",
       " u'abirritate',\n",
       " u'abirritated',\n",
       " u'abirritates',\n",
       " u'abirritating',\n",
       " u'abiturient',\n",
       " u'abiturients',\n",
       " u'abject',\n",
       " u'abjected',\n",
       " u'abjecting',\n",
       " u'abjection',\n",
       " u'abjections',\n",
       " u'abjectly',\n",
       " u'abjectness',\n",
       " u'abjectnesses',\n",
       " u'abjects',\n",
       " u'abjoint',\n",
       " u'abjointed',\n",
       " u'abjointing',\n",
       " u'abjoints',\n",
       " u'abjunction',\n",
       " u'abjunctions',\n",
       " u'abjuration',\n",
       " u'abjurations',\n",
       " u'abjure',\n",
       " u'abjured',\n",
       " u'abjurer',\n",
       " u'abjurers',\n",
       " u'abjures',\n",
       " u'abjuring',\n",
       " u'ablactation',\n",
       " u'ablactations',\n",
       " u'ablate',\n",
       " u'ablated',\n",
       " u'ablates',\n",
       " u'ablating',\n",
       " u'ablation',\n",
       " u'ablations',\n",
       " u'ablatitious',\n",
       " u'ablatival',\n",
       " u'ablative',\n",
       " u'ablatively',\n",
       " u'ablatives',\n",
       " u'ablator',\n",
       " u'ablators',\n",
       " u'ablaut',\n",
       " u'ablauts',\n",
       " u'ablaze',\n",
       " u'able',\n",
       " u'abled',\n",
       " u'ablegate',\n",
       " u'ablegates',\n",
       " u'ableism',\n",
       " u'ableisms',\n",
       " u'ableist',\n",
       " u'ableists',\n",
       " u'abler',\n",
       " u'ables',\n",
       " u'ablest',\n",
       " u'ablet',\n",
       " u'ablets',\n",
       " u'abling',\n",
       " u'ablings',\n",
       " u'ablins',\n",
       " u'abloom',\n",
       " u'ablow',\n",
       " u'abluent',\n",
       " u'abluents',\n",
       " u'ablush',\n",
       " u'abluted',\n",
       " u'ablution',\n",
       " u'ablutionary',\n",
       " u'ablutions',\n",
       " u'ablutomane',\n",
       " u'ablutomanes',\n",
       " u'ably',\n",
       " u'abmho',\n",
       " u'abmhos',\n",
       " u'abnegate',\n",
       " u'abnegated',\n",
       " u'abnegates',\n",
       " u'abnegating',\n",
       " u'abnegation',\n",
       " u'abnegations',\n",
       " u'abnegator',\n",
       " u'abnegators',\n",
       " u'abnormal',\n",
       " u'abnormalism',\n",
       " u'abnormalisms',\n",
       " u'abnormalities',\n",
       " u'abnormality',\n",
       " u'abnormally',\n",
       " u'abnormals',\n",
       " u'abnormities',\n",
       " u'abnormity',\n",
       " u'abnormous',\n",
       " u'abo',\n",
       " u'aboard',\n",
       " u'abode',\n",
       " u'aboded',\n",
       " u'abodement',\n",
       " u'abodements',\n",
       " u'abodes',\n",
       " u'aboding',\n",
       " u'abohm',\n",
       " u'abohms',\n",
       " u'aboideau',\n",
       " u'aboideaus',\n",
       " u'aboideaux',\n",
       " u'aboil',\n",
       " u'aboiteau',\n",
       " u'aboiteaus',\n",
       " u'aboiteaux',\n",
       " u'abolish',\n",
       " u'abolishable',\n",
       " u'abolished',\n",
       " u'abolisher',\n",
       " u'abolishers',\n",
       " u'abolishes',\n",
       " u'abolishing',\n",
       " u'abolishment',\n",
       " u'abolishments',\n",
       " u'abolition',\n",
       " u'abolitional',\n",
       " u'abolitionary',\n",
       " u'abolitionism',\n",
       " u'abolitionisms',\n",
       " u'abolitionist',\n",
       " u'abolitionists',\n",
       " u'abolitions',\n",
       " u'abolla',\n",
       " u'abollae',\n",
       " u'abollas',\n",
       " u'aboma',\n",
       " u'abomas',\n",
       " u'abomasa',\n",
       " u'abomasal',\n",
       " u'abomasi',\n",
       " u'abomasum',\n",
       " u'abomasus',\n",
       " u'abomasuses',\n",
       " u'abominable',\n",
       " u'abominableness',\n",
       " u'abominably',\n",
       " u'abominate',\n",
       " u'abominated',\n",
       " u'abominates',\n",
       " u'abominating',\n",
       " u'abomination',\n",
       " u'abominations',\n",
       " u'abominator',\n",
       " u'abominators',\n",
       " u'abondance',\n",
       " u'abondances',\n",
       " u'abonnement',\n",
       " u'abonnements',\n",
       " u'aboon',\n",
       " u'aboral',\n",
       " u'aborally',\n",
       " u'abord',\n",
       " u'aborded',\n",
       " u'abording',\n",
       " u'abords',\n",
       " u'abore',\n",
       " u'aborigen',\n",
       " u'aborigens',\n",
       " u'aborigin',\n",
       " u'aboriginal',\n",
       " u'aboriginalism',\n",
       " u'aboriginalisms',\n",
       " u'aboriginalities',\n",
       " u'aboriginality',\n",
       " u'aboriginally',\n",
       " u'aboriginals',\n",
       " u'aborigine',\n",
       " u'aborigines',\n",
       " u'aborigins',\n",
       " u'aborne',\n",
       " u'aborning',\n",
       " u'abort',\n",
       " u'aborted',\n",
       " u'abortee',\n",
       " u'abortees',\n",
       " u'aborter',\n",
       " u'aborters',\n",
       " u'aborticide',\n",
       " u'aborticides',\n",
       " u'abortifacient',\n",
       " u'abortifacients',\n",
       " u'aborting',\n",
       " u'abortion',\n",
       " u'abortional',\n",
       " u'abortionist',\n",
       " u'abortionists',\n",
       " u'abortions',\n",
       " u'abortive',\n",
       " u'abortively',\n",
       " u'abortiveness',\n",
       " u'abortivenesses',\n",
       " u'aborts',\n",
       " u'abortuaries',\n",
       " u'abortuary',\n",
       " u'abortus',\n",
       " u'abortuses',\n",
       " u'abos',\n",
       " u'abought',\n",
       " u'aboulia',\n",
       " u'aboulias',\n",
       " u'aboulic',\n",
       " u'abound',\n",
       " u'abounded',\n",
       " u'abounding',\n",
       " u'abounds',\n",
       " u'about',\n",
       " u'abouts',\n",
       " u'above',\n",
       " u'aboveboard',\n",
       " u'aboveground',\n",
       " u'aboves',\n",
       " u'abracadabra',\n",
       " u'abracadabras',\n",
       " u'abrachia',\n",
       " u'abrachias',\n",
       " u'abradable',\n",
       " u'abradant',\n",
       " u'abradants',\n",
       " u'abrade',\n",
       " u'abraded',\n",
       " u'abrader',\n",
       " u'abraders',\n",
       " u'abrades',\n",
       " u'abrading',\n",
       " u'abraid',\n",
       " u'abraided',\n",
       " u'abraiding',\n",
       " u'abraids',\n",
       " u'abram',\n",
       " u'abranchial',\n",
       " u'abranchiate',\n",
       " u'abrasax',\n",
       " u'abrasaxes',\n",
       " u'abrasion',\n",
       " u'abrasions',\n",
       " u'abrasive',\n",
       " u'abrasively',\n",
       " u'abrasiveness',\n",
       " u'abrasivenesses',\n",
       " u'abrasives',\n",
       " u'abraxas',\n",
       " u'abraxases',\n",
       " u'abray',\n",
       " u'abrayed',\n",
       " u'abraying',\n",
       " u'abrays',\n",
       " u'abrazo',\n",
       " u'abrazos',\n",
       " u'abreact',\n",
       " u'abreacted',\n",
       " u'abreacting',\n",
       " u'abreaction',\n",
       " u'abreactions',\n",
       " u'abreactive',\n",
       " u'abreacts',\n",
       " u'abreast',\n",
       " u'abrege',\n",
       " u'abreges',\n",
       " u'abri',\n",
       " u'abricock',\n",
       " u'abricocks',\n",
       " u'abridgable',\n",
       " u'abridge',\n",
       " u'abridgeable',\n",
       " u'abridged',\n",
       " u'abridgement',\n",
       " u'abridgements',\n",
       " u'abridger',\n",
       " u'abridgers',\n",
       " u'abridges',\n",
       " u'abridging',\n",
       " u'abridgment',\n",
       " u'abridgments',\n",
       " u'abrim',\n",
       " u'abrin',\n",
       " u'abrins',\n",
       " u'abris',\n",
       " u'abroach',\n",
       " u'abroad',\n",
       " u'abroads',\n",
       " u'abrogable',\n",
       " u'abrogate',\n",
       " u'abrogated',\n",
       " u'abrogates',\n",
       " u'abrogating',\n",
       " u'abrogation',\n",
       " u'abrogations',\n",
       " u'abrogative',\n",
       " u'abrogator',\n",
       " u'abrogators',\n",
       " u'abrooke',\n",
       " u'abrooked',\n",
       " u'abrookes',\n",
       " u'abrooking',\n",
       " u'abrosia',\n",
       " u'abrosias',\n",
       " u'abrupt',\n",
       " u'abrupter',\n",
       " u'abruptest',\n",
       " u'abruption',\n",
       " u'abruptions',\n",
       " u'abruptly',\n",
       " u'abruptness',\n",
       " u'abruptnesses',\n",
       " u'abrupts',\n",
       " u'abs',\n",
       " u'abscess',\n",
       " u'abscessed',\n",
       " u'abscesses',\n",
       " u'abscessing',\n",
       " u'abscind',\n",
       " u'abscinded',\n",
       " u'abscinding',\n",
       " u'abscinds',\n",
       " u'abscise',\n",
       " u'abscised',\n",
       " u'abscises',\n",
       " u'abscisin',\n",
       " u'abscising',\n",
       " u'abscisins',\n",
       " u'absciss',\n",
       " u'abscissa',\n",
       " u'abscissae',\n",
       " u'abscissas',\n",
       " u'abscisse',\n",
       " u'abscisses',\n",
       " u'abscissin',\n",
       " u'abscissins',\n",
       " u'abscission',\n",
       " u'abscissions',\n",
       " u'abscond',\n",
       " u'absconded',\n",
       " u'abscondence',\n",
       " u'abscondences',\n",
       " u'absconder',\n",
       " u'absconders',\n",
       " u'absconding',\n",
       " u'absconds',\n",
       " u'abseil',\n",
       " u'abseiled',\n",
       " u'abseiling',\n",
       " u'abseilings',\n",
       " u'abseils',\n",
       " u'absence',\n",
       " u'absences',\n",
       " u'absent',\n",
       " u'absented',\n",
       " u'absentee',\n",
       " u'absenteeism',\n",
       " u'absenteeisms',\n",
       " u'absentees',\n",
       " u'absenter',\n",
       " u'absenters',\n",
       " u'absenting',\n",
       " u'absently',\n",
       " u'absentminded',\n",
       " u'absentmindedly',\n",
       " u'absents',\n",
       " u'absey',\n",
       " u'abseys',\n",
       " u'absinth',\n",
       " u'absinthe',\n",
       " u'absinthes',\n",
       " u'absinthiated',\n",
       " u'absinthism',\n",
       " u'absinthisms',\n",
       " u'absinths',\n",
       " u'absit',\n",
       " u'absits',\n",
       " u'absolute',\n",
       " u'absolutely',\n",
       " u'absoluteness',\n",
       " u'absolutenesses',\n",
       " u'absoluter',\n",
       " u'absolutes',\n",
       " u'absolutest',\n",
       " u'absolution',\n",
       " u'absolutions',\n",
       " u'absolutise',\n",
       " u'absolutised',\n",
       " u'absolutises',\n",
       " u'absolutising',\n",
       " u'absolutism',\n",
       " u'absolutisms',\n",
       " u'absolutist',\n",
       " u'absolutistic',\n",
       " u'absolutists',\n",
       " u'absolutive',\n",
       " u'absolutize',\n",
       " u'absolutized',\n",
       " u'absolutizes',\n",
       " u'absolutizing',\n",
       " u'absolutory',\n",
       " u'absolvable',\n",
       " u'absolve',\n",
       " u'absolved',\n",
       " u'absolvent',\n",
       " u'absolvents',\n",
       " u'absolver',\n",
       " u'absolvers',\n",
       " u'absolves',\n",
       " u'absolving',\n",
       " u'absolvitor',\n",
       " u'absolvitors',\n",
       " u'absonant',\n",
       " u'absorb',\n",
       " u'absorbabilities',\n",
       " u'absorbability',\n",
       " u'absorbable',\n",
       " u'absorbance',\n",
       " u'absorbances',\n",
       " u'absorbancies',\n",
       " u'absorbancy',\n",
       " u'absorbant',\n",
       " u'absorbants',\n",
       " u'absorbate',\n",
       " u'absorbates',\n",
       " u'absorbed',\n",
       " u'absorbedly',\n",
       " u'absorbefacient',\n",
       " u'absorbefacients',\n",
       " u'absorbencies',\n",
       " u'absorbency',\n",
       " u'absorbent',\n",
       " u'absorbents',\n",
       " u'absorber',\n",
       " u'absorbers',\n",
       " u'absorbing',\n",
       " u'absorbingly',\n",
       " u'absorbs',\n",
       " u'absorptance',\n",
       " u'absorptances',\n",
       " u'absorptiometer',\n",
       " u'absorptiometers',\n",
       " u'absorption',\n",
       " u'absorptions',\n",
       " u'absorptive',\n",
       " u'absorptiveness',\n",
       " u'absorptivities',\n",
       " u'absorptivity',\n",
       " u'absquatulate',\n",
       " u'absquatulated',\n",
       " u'absquatulates',\n",
       " u'absquatulating',\n",
       " u'abstain',\n",
       " u'abstained',\n",
       " u'abstainer',\n",
       " u'abstainers',\n",
       " u'abstaining',\n",
       " u'abstains',\n",
       " u'abstemious',\n",
       " u'abstemiously',\n",
       " u'abstemiousness',\n",
       " u'abstention',\n",
       " u'abstentionism',\n",
       " u'abstentionisms',\n",
       " u'abstentionist',\n",
       " u'abstentionists',\n",
       " u'abstentions',\n",
       " u'abstentious',\n",
       " u'absterge',\n",
       " u'absterged',\n",
       " u'abstergent',\n",
       " u'abstergents',\n",
       " u'absterges',\n",
       " u'absterging',\n",
       " u'abstersion',\n",
       " u'abstersions',\n",
       " u'abstersive',\n",
       " u'abstersives',\n",
       " u'abstinence',\n",
       " u'abstinences',\n",
       " u'abstinencies',\n",
       " u'abstinency',\n",
       " u'abstinent',\n",
       " u'abstinently',\n",
       " u'abstract',\n",
       " u'abstractable',\n",
       " u'abstracted',\n",
       " u'abstractedly',\n",
       " u'abstractedness',\n",
       " u'abstracter',\n",
       " u'abstracters',\n",
       " u'abstractest',\n",
       " u'abstracting',\n",
       " u'abstraction',\n",
       " u'abstractional',\n",
       " u'abstractionism',\n",
       " u'abstractionisms',\n",
       " u'abstractionist',\n",
       " u'abstractionists',\n",
       " u'abstractions',\n",
       " u'abstractive',\n",
       " u'abstractively',\n",
       " u'abstractives',\n",
       " u'abstractly',\n",
       " u'abstractness',\n",
       " u'abstractnesses',\n",
       " u'abstractor',\n",
       " u'abstractors',\n",
       " u'abstracts',\n",
       " u'abstrict',\n",
       " u'abstricted',\n",
       " u'abstricting',\n",
       " u'abstriction',\n",
       " u'abstrictions',\n",
       " u'abstricts',\n",
       " u'abstruse',\n",
       " u'abstrusely',\n",
       " u'abstruseness',\n",
       " u'abstrusenesses',\n",
       " u'abstruser',\n",
       " u'abstrusest',\n",
       " u'abstrusities',\n",
       " u'abstrusity',\n",
       " u'absurd',\n",
       " u'absurder',\n",
       " u'absurdest',\n",
       " u'absurdism',\n",
       " u'absurdisms',\n",
       " u'absurdist',\n",
       " u'absurdists',\n",
       " u'absurdities',\n",
       " u'absurdity',\n",
       " u'absurdly',\n",
       " u'absurdness',\n",
       " u'absurdnesses',\n",
       " u'absurds',\n",
       " u'abthane',\n",
       " u'abthanes',\n",
       " u'abubble',\n",
       " u'abuilding',\n",
       " u'abulia',\n",
       " u'abulias',\n",
       " u'abulic',\n",
       " u'abuna',\n",
       " u'abunas',\n",
       " u'abundance',\n",
       " u'abundances',\n",
       " u'abundancies',\n",
       " u'abundancy',\n",
       " u'abundant',\n",
       " u'abundantly',\n",
       " u'abune',\n",
       " u'aburst',\n",
       " u'abusable',\n",
       " u'abusage',\n",
       " u'abusages',\n",
       " u'abuse',\n",
       " u'abused',\n",
       " u'abuser',\n",
       " u'abusers',\n",
       " u'abuses',\n",
       " u'abusing',\n",
       " u'abusion',\n",
       " u'abusions',\n",
       " u'abusive',\n",
       " u'abusively',\n",
       " u'abusiveness',\n",
       " u'abusivenesses',\n",
       " u'abut',\n",
       " u'abutilon',\n",
       " u'abutilons',\n",
       " u'abutment',\n",
       " u'abutments',\n",
       " u'abuts',\n",
       " u'abuttal',\n",
       " u'abuttals',\n",
       " u'abutted',\n",
       " u'abutter',\n",
       " u'abutters',\n",
       " u'abutting',\n",
       " u'abuzz',\n",
       " u'abvolt',\n",
       " u'abvolts',\n",
       " u'abwatt',\n",
       " u'abwatts',\n",
       " u'aby',\n",
       " u'abye',\n",
       " u'abyeing',\n",
       " u'abyes',\n",
       " u'abying',\n",
       " u'abys',\n",
       " u'abysm',\n",
       " u'abysmal',\n",
       " u'abysmally',\n",
       " u'abysms',\n",
       " u'abyss',\n",
       " u'abyssal',\n",
       " u'abysses',\n",
       " u'abyssopelagic',\n",
       " u'acacia',\n",
       " u'acacias',\n",
       " u'academe',\n",
       " u'academes',\n",
       " u'academia',\n",
       " u'academias',\n",
       " u'academic',\n",
       " u'academical',\n",
       " u'academicalism',\n",
       " u'academicalisms',\n",
       " u'academically',\n",
       " u'academicals',\n",
       " u'academician',\n",
       " u'academicians',\n",
       " u'academicism',\n",
       " u'academicisms',\n",
       " u'academics',\n",
       " u'academies',\n",
       " u'academism',\n",
       " u'academisms',\n",
       " u'academist',\n",
       " u'academists',\n",
       " u'academy',\n",
       " u'acai',\n",
       " u'acais',\n",
       " u'acajou',\n",
       " u'acajous',\n",
       " u'acalculia',\n",
       " u'acalculias',\n",
       " u'acaleph',\n",
       " u'acalephae',\n",
       " u'acalephan',\n",
       " u'acalephans',\n",
       " u'acalephe',\n",
       " u'acalephes',\n",
       " u'acalephs',\n",
       " u'acanaceous',\n",
       " u'acanth',\n",
       " u'acantha',\n",
       " u'acanthaceous',\n",
       " u'acanthae',\n",
       " u'acanthas',\n",
       " u'acanthi',\n",
       " u'acanthin',\n",
       " u'acanthine',\n",
       " u'acanthins',\n",
       " u'acanthocephalan',\n",
       " u'acanthoid',\n",
       " u'acanthous',\n",
       " u'acanths',\n",
       " u'acanthus',\n",
       " u'acanthuses',\n",
       " u'acapnia',\n",
       " u'acapnias',\n",
       " u'acarbose',\n",
       " u'acarboses',\n",
       " u'acari',\n",
       " u'acarian',\n",
       " u'acariases',\n",
       " u'acariasis',\n",
       " u'acaricidal',\n",
       " u'acaricide',\n",
       " u'acaricides',\n",
       " u'acarid',\n",
       " u'acaridan',\n",
       " u'acaridans',\n",
       " u'acaridean',\n",
       " u'acarideans',\n",
       " u'acaridian',\n",
       " u'acaridians',\n",
       " u'acaridomatia',\n",
       " u'acaridomatium',\n",
       " u'acarids',\n",
       " u'acarine',\n",
       " u'acarines',\n",
       " u'acarodomatia',\n",
       " u'acarodomatium',\n",
       " u'acaroid',\n",
       " u'acarologies',\n",
       " u'acarologist',\n",
       " u'acarologists',\n",
       " u'acarology',\n",
       " u'acarophilies',\n",
       " u'acarophily',\n",
       " u'acarpellous',\n",
       " u'acarpelous',\n",
       " u'acarpous',\n",
       " u'acarus',\n",
       " u'acatalectic',\n",
       " u'acatalectics',\n",
       " u'acatalepsies',\n",
       " u'acatalepsy',\n",
       " u'acataleptic',\n",
       " u'acataleptics',\n",
       " u'acatamathesia',\n",
       " u'acatamathesias',\n",
       " u'acater',\n",
       " u'acaters',\n",
       " u'acates',\n",
       " u'acatour',\n",
       " u'acatours',\n",
       " u'acaudal',\n",
       " u'acaudate',\n",
       " u'acaulescent',\n",
       " u'acauline',\n",
       " u'acaulose',\n",
       " u'acaulous',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sowpods_rdd\n",
    "    # Your code goes here\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 3 - Lazy execution ###\n",
    "\n",
    "Check out this function. It works for string numbers and breaks for other characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'g'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3ee3a41425fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-3ee3a41425fa>\u001b[0m in \u001b[0;36mparse_int\u001b[0;34m(my_char)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'g'"
     ]
    }
   ],
   "source": [
    "def parse_int(my_char):\n",
    "    return int(my_char)\n",
    "\n",
    "print parse_int(\"5\")\n",
    "print parse_int(\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Why does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = list('0123456789a')\n",
    "my_rdd = sc.parallelize(my_list)\n",
    "\n",
    "my_rdd.map(parse_int).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "...this fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 41.0 failed 1 times, most recent failure: Lost task 1.0 in stage 41.0 (TID 55, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-3ee3a41425fa>\", line 2, in parse_int\nValueError: invalid literal for int() with base 10: 'a'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-3ee3a41425fa>\", line 2, in parse_int\nValueError: invalid literal for int() with base 10: 'a'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3f259b021684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 41.0 failed 1 times, most recent failure: Lost task 1.0 in stage 41.0 (TID 55, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-3ee3a41425fa>\", line 2, in parse_int\nValueError: invalid literal for int() with base 10: 'a'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-3ee3a41425fa>\", line 2, in parse_int\nValueError: invalid literal for int() with base 10: 'a'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "my_rdd.map(parse_int).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "...and this *sometimes* fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 42.0 failed 1 times, most recent failure: Lost task 1.0 in stage 42.0 (TID 57, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-3ee3a41425fa>\", line 2, in parse_int\nValueError: invalid literal for int() with base 10: 'a'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-3ee3a41425fa>\", line 2, in parse_int\nValueError: invalid literal for int() with base 10: 'a'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f3fb9fee58a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 42.0 failed 1 times, most recent failure: Lost task 1.0 in stage 42.0 (TID 57, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-3ee3a41425fa>\", line 2, in parse_int\nValueError: invalid literal for int() with base 10: 'a'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/ceoxxx/spark/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-3ee3a41425fa>\", line 2, in parse_int\nValueError: invalid literal for int() with base 10: 'a'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "my_rdd.sample(False, .5).map(parse_int).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bonus Exercise ##\n",
    "\n",
    "Let's up the stakes. For this exercise, regular expressions are *not allowed.*\n",
    "- How many SOWPODS words contain double 'aa's?\n",
    "- How about 'bb', 'cc', 'dd', ..., 'zz'?\n",
    "- There are 3 SOWPODS words that contain triple letters. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
